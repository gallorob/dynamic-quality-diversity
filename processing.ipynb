{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "column_to_metric = {\n",
    "    'Environment': 'env', 'Algorithm': 'alg', 'Policy': 'policy', 'Sampling': 'custom_sampling',\n",
    "    'Detection': 'detection_on', 'Re-Evaluation': 'reevaluate_on',\n",
    "\n",
    "    \"Mean (MSE Objective)\": \"Mean ± Standard Deviation (MSE Objective)\",\n",
    "    \"Mean (MSE BC 1)\": \"Mean ± Standard Deviation (MSE BC 1)\",\n",
    "    \"Mean (MSE BC 2)\": \"Mean ± Standard Deviation (MSE BC 2)\",\n",
    "    \"Mean (MSE QD)\": \"Mean ± Standard Deviation (MSE QD)\",\n",
    "\n",
    "    \"AUC (MSE Objective)\": \"AUC (MSE Objective)\",\n",
    "    \"AUC (MSE BC 1)\": \"AUC (MSE BC 1)\",\n",
    "    \"AUC (MSE BC 2)\": \"AUC (MSE BC 2)\",\n",
    "    \"AUC (MSE QD)\": \"AUC (MSE QD)\",\n",
    "    \"Mean (Survival %)\": \"Survival (%)\",\n",
    "    \"Evaluations\": \"Evaluations\",\n",
    "\n",
    "    \"Rate of Change (MSE Objective)\": \"Offline Error (Objective)\",\n",
    "    \"Rate of Change (MSE QD)\": \"Offline Error (QD)\",\n",
    "    \n",
    "    \"Mean Evaluations For Survival (50%)\": None,\n",
    "    \"Mean Evaluations For Survival (75%)\": None,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fc23f703e4fb81",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "results_fname = './stats_results.csv'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f80bc558215a71b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "get_mean = lambda x, settings: x[0]\n",
    "get_std = lambda x, settings: x[1]\n",
    "compute_sum = lambda x, settings: np.cumsum(x)[-1]\n",
    "compute_mean = lambda x, settings: np.mean(x)\n",
    "compute_std = lambda x, settings: np.std(x)\n",
    "get_sample_method = lambda x, settings: settings['sampling_strategy'] if 'sampling_strategy' in settings.keys() else 'custom' if x else 'default'\n",
    "\n",
    "def get_rate_of_change(values, settings):\n",
    "    t = settings['time_shift_val']\n",
    "    v = np.asarray(values)\n",
    "    a = v[0:len(values):t]\n",
    "    b = v[t-1:len(values):t]\n",
    "    return np.mean(b - a)\n",
    "\n",
    "def get_survival_score_over_threshold(results, settings, threshold = 50.0, use_actual=False):\n",
    "    evals = np.asarray(results['Evaluations'])\n",
    "    survs = np.asarray(results['Survival (%)'])\n",
    "    # detected_shifts = np.asarray(results['Detected Shift']) if not use_actual else np.asarray(results['Actual Shift'])\n",
    "    detected_shifts = np.asarray(results['Actual Shift'])\n",
    "    n_detected = np.sum(detected_shifts)\n",
    "    _scores = [0]\n",
    "    _evals = [np.nan]\n",
    "    \n",
    "    detected_shifts = np.where(detected_shifts)[0]\n",
    "    \n",
    "    for i in range(len(detected_shifts) - 1):\n",
    "        bf, af = detected_shifts[i], detected_shifts[i + 1]\n",
    "        if np.any(survs[bf:af] > threshold):\n",
    "            _scores.append(1)\n",
    "            _evals.append(np.sum(evals[bf:af]))\n",
    "        else:\n",
    "            _scores.append(0)\n",
    "            _evals.append(np.nan)\n",
    "    \n",
    "    return np.sum(_scores) / n_detected, np.nanmean(_evals)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7df28d76eacbb549",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "column_to_op = {\n",
    "    'Sampling': get_sample_method,\n",
    "\n",
    "    \"Mean (MSE Objective)\": get_mean, \"Standard Deviation (MSE Objective)\": get_std,\n",
    "    \"Mean (MSE BC 1)\": get_mean, \"Standard Deviation (MSE BC 1)\": get_std,\n",
    "    \"Mean (MSE BC 2)\": get_mean, \"Standard Deviation (MSE BC 2)\": get_std,\n",
    "    \"Mean (MSE QD)\": get_mean, \"Standard Deviation (MSE QD)\": get_std,\n",
    "    \"Mean (Survival %)\": compute_mean, \"Standard Deviation (Survival %)\": compute_std,\n",
    "    \"Rate of Change (MSE Objective)\": get_rate_of_change,\n",
    "    \"Rate of Change (MSE QD)\": get_rate_of_change,\n",
    "\n",
    "    \"Evaluations\": compute_sum\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8e2a5897f7a5173",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "ref_dir = './raw_results'\n",
    "\n",
    "items = os.listdir(ref_dir)\n",
    "folders = [item for item in items if os.path.isdir(os.path.join(ref_dir, item))]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c6d64a233b725f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "with trange(0, len(folders), desc='Processing experiments...') as ii:\n",
    "    for i in ii:\n",
    "        try:\n",
    "            experiment_fname = folders[i]\n",
    "            ii.set_postfix_str(f'{experiment_fname}', refresh=True)\n",
    "            experiment_path = os.path.join(ref_dir, experiment_fname)\n",
    "            with open(os.path.join(experiment_path, f'settings.metadata'), 'r') as f:\n",
    "                settings = json.load(f)\n",
    "            with open(os.path.join(experiment_path, 'results.json'), 'r') as f:\n",
    "                results = json.load(f)\n",
    "            env, alg, policies = settings['env'], settings['alg'], settings['policies']\n",
    "            for policy in policies:\n",
    "                new_row = {'run': experiment_fname.split('_')[1]}\n",
    "                for c in column_to_metric.keys():\n",
    "                    if c == 'Policy':\n",
    "                        m = policy\n",
    "                    elif c == \"Mean Evaluations For Survival (50%)\":\n",
    "                        m = get_survival_score_over_threshold(results[policy], settings, 50.0, policy=='no_updates')\n",
    "                    elif c == \"Mean Evaluations For Survival (75%)\":\n",
    "                        m = get_survival_score_over_threshold(results[policy], settings, 75.0, policy=='no_updates')\n",
    "                    elif column_to_metric[c] in settings.keys():\n",
    "                        m = settings[column_to_metric[c]]\n",
    "                    else:\n",
    "                        m = results[policy][column_to_metric[c]]\n",
    "                    if c in column_to_op.keys():\n",
    "                        m = column_to_op[c](m, settings)\n",
    "                    new_row.update({c: m})\n",
    "                df2 = pd.DataFrame([new_row])\n",
    "                df = pd.concat([df, df2], ignore_index=True)\n",
    "        except Exception:\n",
    "            print(f'Skipped {experiment_fname}...')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76879aad4868343c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.to_csv('grouped_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5482a84d4613c156",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('grouped_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540999d54e8f34a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "envs = df['Environment'].unique()\n",
    "algs = df['Algorithm'].unique()\n",
    "policies = list(sorted(df['Policy'].unique()))\n",
    "samplings = list(sorted(df['Sampling'].unique()))\n",
    "detections = list(sorted(df['Detection'].unique()))\n",
    "reevaluations = list(sorted(df['Re-Evaluation'].unique()))\n",
    "\n",
    "shared_columns = ['run', 'Environment', 'Algorithm', 'Policy', 'Sampling', 'Detection', 'Re-Evaluation']\n",
    "\n",
    "metrics = df.columns.drop(shared_columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d96726a4c13cb3f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "processed_df = pd.DataFrame()\n",
    "\n",
    "for env in envs:\n",
    "    for alg in algs:\n",
    "        for policy in policies:\n",
    "            for sampling in samplings:\n",
    "                for detection in detections:\n",
    "                    for reevaluation in reevaluations:\n",
    "                        values = df.loc[(df['Environment'] == env) & (df['Algorithm'] == alg) \\\n",
    "                                            & (df['Policy'] == policy) & (df['Sampling'] == sampling) \\\n",
    "                                            & (df['Detection'] == detection) & (df['Re-Evaluation'] == reevaluation)]\n",
    "                        if len(values) == 0: continue\n",
    "                        if (policy == 'no_updates' or policy == 'update_all') and sampling != 'default': continue\n",
    "                        mean_values, std_values = {}, {}\n",
    "                        for metric in metrics:\n",
    "                            if metric.startswith(\"Mean Evaluations For Survival\"):\n",
    "                                mean_scores = np.nanmean(np.sum([x[1] for x in values[metric]]))\n",
    "                                std_scores = np.nanstd(np.sum([x[1] for x in values[metric]]))\n",
    "                                mean_iterations = np.nanmean([x[0] for x in values[metric]])\n",
    "                                std_iterations = np.nanstd([x[0] for x in values[metric]])\n",
    "                                mean_values[f'Mean {metric}'] = (mean_scores, mean_iterations)\n",
    "                                std_values[f'Std {metric}'] = (std_scores, std_iterations)\n",
    "                            else:\n",
    "                                mean_values[f'Mean {metric}'] = values[metric].mean()\n",
    "                                std_values[f'Std {metric}'] = values[metric].std()\n",
    "                        # ci_value = {f'95CI {metric}': st.t.interval(0.95, len(values[metric]) - 1, loc=values[metric].mean(), scale=st.sem(values[metric])) for metric in metrics}\n",
    "                        new_row = pd.DataFrame([{'Environment': env, 'Algorithm': alg, 'Policy': policy,\n",
    "                                                 'Sampling': sampling, 'Detection': detection, 'Re-Evaluation': reevaluation,\n",
    "                                                 **mean_values, **std_values}])\n",
    "                        processed_df = pd.concat([processed_df, new_row], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48af00a54edf7e5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "processed_df.to_csv('statistical_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3299b5f59a825fac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "processed_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "851e7e58a42f1b6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "processed_df = pd.read_csv('statistical_data.csv')\n",
    "results_fname = 'arxiv_results.csv'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb556ade6a2e917a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def run_ttests(stats_testing_values, metrics_of_interest):\n",
    "    all_configs = list(stats_testing_values.keys())\n",
    "    \n",
    "    config_combinations = {}\n",
    "    for config in all_configs:\n",
    "        config_combinations[config] = []\n",
    "        curr_env, curr_alg = config.split('__')[0], config.split('__')[1] \n",
    "        for other_config in all_configs:\n",
    "            if config != other_config and curr_env == other_config.split('__')[0] and curr_alg == other_config.split('__')[1]:\n",
    "                if ('no_updates' in config or 'update_all' in config) and ('no_updates' in other_config or 'update_all' in other_config):\n",
    "                    config_combinations[config].append(other_config)\n",
    "                else:\n",
    "                    config_combinations[config].append(other_config)\n",
    "    \n",
    "    with open(results_fname, 'a+') as f:\n",
    "        for metric in metrics_of_interest:\n",
    "            # f.write(f'### Analysis for {metric} ###\\n')\n",
    "            for config in config_combinations.keys():\n",
    "                if ('no_updates' in config or 'update_all' in config): continue\n",
    "                # f.write(f'\\n{config}\\n')\n",
    "                raw_p_values = []\n",
    "                for other_config in config_combinations[config]:\n",
    "                    if config != other_config:\n",
    "                        _, pv = ttest_ind(stats_testing_values[config][metric], stats_testing_values[other_config][metric])\n",
    "                        raw_p_values.append(pv)\n",
    "                if not ('no_updates' in other_config or 'update_all' in other_config):\n",
    "                    _, corrected_pvs, _, bonfalpha = multipletests(raw_p_values, method='bonferroni')\n",
    "                    p_values = corrected_pvs\n",
    "                else:\n",
    "                    p_values = raw_p_values\n",
    "                \n",
    "                for i, (other_config, p_value) in enumerate(zip(config_combinations[config], p_values)):\n",
    "                    f.write(f'{metric},{config},{other_config},{p_value},{raw_p_values[i] if not (\"no_updates\" in other_config or \"update_all\" in other_config) else \"\"},{p_value < 0.05}')\n",
    "                    f.write('\\n')\n",
    "                    \n",
    "            f.write('')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39d914ca5877ff5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "RQ1: Two tables (one per environment) with Algorithm, Policy, Sampling/Detection/Reeval, Survival percentage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1ca37f7b26b8d1b"
  },
  {
   "cell_type": "code",
   "source": [
    "metrics_of_interest = ['Mean (Survival %)']\n",
    "stats_testing_values = {}\n",
    "\n",
    "rq1_columns = shared_columns.copy()\n",
    "rq1_columns.extend(metrics_of_interest)\n",
    "not_rq1_columns = list(set(list(df.columns)).difference(rq1_columns))\n",
    "rq1_raw_data = df.columns.drop(not_rq1_columns)\n",
    "df[rq1_raw_data].to_csv('rq1.csv')\n",
    "\n",
    "for env in envs:\n",
    "    df_view = pd.DataFrame()\n",
    "    n_runs = 10 if env == 'sphere' else 5\n",
    "\n",
    "    for alg in algs:\n",
    "        for sampling in samplings:\n",
    "            for policy in policies:\n",
    "                for detection in detections:\n",
    "                    for reevaluation in reevaluations:\n",
    "                        values = processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                            & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                            & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)]\n",
    "                        if len(values) == 0: continue\n",
    "                        stats_testing_values['__'.join([env, alg, sampling, policy, detection, reevaluation])] = {metric: df.loc[(df['Environment'] == env) & (df['Algorithm'] == alg) \\\n",
    "                                                                                                                       & (df['Policy'] == policy) & (df['Sampling'] == sampling) \\\n",
    "                                                                                                                       & (df['Detection'] == detection) & (df['Re-Evaluation'] == reevaluation)][metric].values for metric in metrics_of_interest}\n",
    "                        values_of_interest = {metric: f'${values[f\"Mean {metric}\"].values[0]:.3f} \\\\pm {(1.96 * values[f\"Std {metric}\"].values[0]) / np.sqrt(n_runs):.3f}$' for metric in metrics_of_interest}\n",
    "                        new_row = pd.DataFrame([{'Environment': env, 'Algorithm': alg, 'Policy': policy,\n",
    "                                                 'Sampling': sampling, 'Detection': detection, 'Re-Evaluation': reevaluation,\n",
    "                                                 **values_of_interest}])\n",
    "                        df_view = pd.concat([df_view, new_row])\n",
    "\n",
    "    print(df_view.to_latex(index=False))\n",
    "\n",
    "    run_ttests(stats_testing_values, metrics_of_interest)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e1ec71a06787476",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "RQ2: Two tables (one per environment) with Algorithm, Policy, Sampling/Detection/Reeval, mean+-std MSEs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4db3489bd23f895"
  },
  {
   "cell_type": "code",
   "source": [
    "metrics_of_interest = ['Mean (MSE Objective)', 'Mean (MSE BC 1)', 'Mean (MSE BC 2)', 'Mean (MSE QD)']\n",
    "stats_testing_values = {}\n",
    "\n",
    "rq2_columns = shared_columns.copy()\n",
    "rq2_columns.extend(metrics_of_interest)\n",
    "not_rq2_columns = list(set(list(df.columns)).difference(rq2_columns))\n",
    "rq2_raw_data = df.columns.drop(not_rq2_columns)\n",
    "df[rq2_raw_data].to_csv('rq2.csv')\n",
    "\n",
    "for env in envs:\n",
    "    df_view = pd.DataFrame()\n",
    "    n_runs = 10 if env == 'sphere' else 5\n",
    "\n",
    "    for alg in algs:\n",
    "        for sampling in samplings:\n",
    "            for policy in policies:\n",
    "                for detection in detections:\n",
    "                    for reevaluation in reevaluations:\n",
    "                        values = processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                            & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                            & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)]\n",
    "                        if len(values) == 0: continue\n",
    "                        stats_testing_values['__'.join([env, alg, sampling, policy, detection, reevaluation])] = {metric: df.loc[(df['Environment'] == env) & (df['Algorithm'] == alg) \\\n",
    "                                                                                                                       & (df['Policy'] == policy) & (df['Sampling'] == sampling) \\\n",
    "                                                                                                                       & (df['Detection'] == detection) & (df['Re-Evaluation'] == reevaluation)][metric].values for metric in metrics_of_interest}\n",
    "                        values_of_interest = {metric: f'${values[f\"Mean {metric}\"].values[0]:.3f} \\\\pm {1.96 * (values[f\"Std {metric}\"].values[0] / np.sqrt(n_runs)):.3f}$' for metric in metrics_of_interest}\n",
    "                        new_row = pd.DataFrame([{'Environment': env, 'Algorithm': alg, 'Policy': policy,\n",
    "                                                 'Sampling': sampling, 'Detection': detection, 'Re-Evaluation': reevaluation,\n",
    "                                                 **values_of_interest}])\n",
    "                        df_view = pd.concat([df_view, new_row])\n",
    "\n",
    "    print(df_view.to_latex(index=False))\n",
    "\n",
    "    run_ttests(stats_testing_values, metrics_of_interest)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83f9f842450befa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "RQ3: Two tables (one per environment) with Algorithm, Policy, Sampling/Detection/Reeval, ROCs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce6ad85096966c8c"
  },
  {
   "cell_type": "code",
   "source": [
    "metrics_of_interest = ['Rate of Change (MSE Objective)', 'Rate of Change (MSE QD)']\n",
    "stats_testing_values = {}\n",
    "\n",
    "rq3_columns = shared_columns.copy()\n",
    "rq3_columns.extend(metrics_of_interest)\n",
    "not_rq3_columns = list(set(list(df.columns)).difference(rq3_columns))\n",
    "rq3_raw_data = df.columns.drop(not_rq3_columns)\n",
    "df[rq3_raw_data].to_csv('rq3.csv')\n",
    "\n",
    "for env in ['lunar-lander']:#envs:\n",
    "    df_view = pd.DataFrame()\n",
    "    n_runs = 10 if env == 'sphere' else 5\n",
    "\n",
    "    for alg in algs:\n",
    "        for sampling in ['default']:# samplings:\n",
    "            for policy in policies:\n",
    "                for detection in detections:\n",
    "                    for reevaluation in reevaluations:\n",
    "                        values = processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                            & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                            & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)]\n",
    "                        if len(values) == 0: continue\n",
    "                        stats_testing_values['__'.join([env, alg, sampling, policy, detection, reevaluation])] = {metric: df.loc[(df['Environment'] == env) & (df['Algorithm'] == alg) \\\n",
    "                                                                                                                       & (df['Policy'] == policy) & (df['Sampling'] == sampling) \\\n",
    "                                                                                                                       & (df['Detection'] == detection) & (df['Re-Evaluation'] == reevaluation)][metric].values for metric in metrics_of_interest}\n",
    "                        values_of_interest = {metric: f'${values[f\"Mean {metric}\"].values[0]:.3f} \\\\pm {1.96 * (values[f\"Std {metric}\"].values[0] / np.sqrt(n_runs)):.3f}$' for metric in metrics_of_interest}\n",
    "                        new_row = pd.DataFrame([{'Environment': env, 'Algorithm': alg, 'Policy': policy,\n",
    "                                                 'Sampling': sampling, 'Detection': detection, 'Re-Evaluation': reevaluation,\n",
    "                                                 **values_of_interest}])\n",
    "                        df_view = pd.concat([df_view, new_row])\n",
    "\n",
    "    print(df_view.to_latex(index=False))\n",
    "\n",
    "    run_ttests(stats_testing_values, metrics_of_interest)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8418b06c66d6906",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "RQ4: Two tables (one per environment) with Algorithm, Policy, Sampling/Detection/Reeval, mean+-std cost metrics (to compute)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "436eb762ea008644"
  },
  {
   "cell_type": "code",
   "source": [
    "metrics_of_interest = [\"Mean Evaluations For Survival (50%)\", \"Mean Evaluations For Survival (75%)\"]\n",
    "stats_testing_values = {}\n",
    "\n",
    "rq4_columns = shared_columns.copy()\n",
    "rq4_columns.extend(metrics_of_interest)\n",
    "not_rq4_columns = list(set(list(df.columns)).difference(rq4_columns))\n",
    "rq4_raw_data = df.columns.drop(not_rq4_columns)\n",
    "df[rq4_raw_data].to_csv('rq4.csv')\n",
    "\n",
    "for env in envs:\n",
    "    df_view = pd.DataFrame()\n",
    "\n",
    "    for alg in algs:\n",
    "        for sampling in samplings:\n",
    "            for policy in policies:\n",
    "                for detection in detections:\n",
    "                    for reevaluation in reevaluations:\n",
    "                        values = processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                            & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                            & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)]\n",
    "                        if len(values) == 0: continue\n",
    "                        stats_testing_values['__'.join([env, alg, sampling, policy, detection, reevaluation])] = {metric: [float(a.replace('(', '').replace(')','').split(', ')[1]) for a in processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                                                                                                       & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                                                                                                       & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)][metric].values[:]] for metric in metrics_of_interest}\n",
    "                        values_of_interest = {metric: f'${values[f\"Mean {metric}\"].values[0][0]:.3f} ({values[f\"Mean {metric}\"].values[0][1]:.2%})$' for metric in metrics_of_interest}\n",
    "                        new_row = pd.DataFrame([{'Environment': env, 'Algorithm': alg, 'Policy': policy,\n",
    "                                                 'Sampling': sampling, 'Detection': detection, 'Re-Evaluation': reevaluation,\n",
    "                                                 **values_of_interest}])\n",
    "                        df_view = pd.concat([df_view, new_row])\n",
    "\n",
    "    print(df_view.to_latex(index=False))\n",
    "\n",
    "    run_ttests(stats_testing_values, metrics_of_interest)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c17accad5f48cc30",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "metrics_of_interest = [\"Mean Evaluations For Survival (50%)\", \"Mean Evaluations For Survival (75%)\"]\n",
    "stats_testing_values = {}\n",
    "\n",
    "processed_df = pd.read_csv('rq4.csv')\n",
    "\n",
    "envs = processed_df['Environment'].unique()\n",
    "algs = processed_df['Algorithm'].unique()\n",
    "policies = list(sorted(processed_df['Policy'].unique()))\n",
    "samplings = list(sorted(processed_df['Sampling'].unique()))\n",
    "detections = list(sorted(processed_df['Detection'].unique()))\n",
    "reevaluations = list(sorted(processed_df['Re-Evaluation'].unique()))\n",
    "\n",
    "shared_columns = ['run', 'Environment', 'Algorithm', 'Policy', 'Sampling', 'Detection', 'Re-Evaluation']\n",
    "\n",
    "metrics = processed_df.columns.drop(shared_columns)\n",
    "\n",
    "results_fname = 'rq4_stats.csv'\n",
    "\n",
    "def run_ttests(stats_testing_values, metrics_of_interest):\n",
    "    all_configs = list(stats_testing_values.keys())\n",
    "    \n",
    "    config_combinations = {}\n",
    "    for config in all_configs:\n",
    "        config_combinations[config] = []\n",
    "        curr_env, curr_alg = config.split('__')[0], config.split('__')[1] \n",
    "        for other_config in all_configs:\n",
    "            if config != other_config and curr_env == other_config.split('__')[0] and curr_alg == other_config.split('__')[1]:\n",
    "                if ('no_updates' in config or 'update_all' in config) and ('no_updates' in other_config or 'update_all' in other_config):\n",
    "                    config_combinations[config].append(other_config)\n",
    "                else:\n",
    "                    config_combinations[config].append(other_config)\n",
    "    \n",
    "    with open(results_fname, 'a+') as f:\n",
    "        for metric in metrics_of_interest:\n",
    "            # f.write(f'### Analysis for {metric} ###\\n')\n",
    "            for config in config_combinations.keys():\n",
    "                if ('no_updates' in config or 'update_all' in config): continue\n",
    "                # f.write(f'\\n{config}\\n')\n",
    "                raw_p_values = []\n",
    "                for other_config in config_combinations[config]:\n",
    "                    if config != other_config:\n",
    "                        _, pv = ttest_ind(stats_testing_values[config][metric], stats_testing_values[other_config][metric])\n",
    "                        raw_p_values.append(pv)\n",
    "                if not ('no_updates' in other_config or 'update_all' in other_config):\n",
    "                    _, corrected_pvs, _, bonfalpha = multipletests(raw_p_values, method='bonferroni')\n",
    "                    p_values = corrected_pvs\n",
    "                else:\n",
    "                    p_values = raw_p_values\n",
    "                \n",
    "                for i, (other_config, p_value) in enumerate(zip(config_combinations[config], p_values)):\n",
    "                    f.write(f'{metric},{config},{other_config},{p_value},{raw_p_values[i] if not (\"no_updates\" in other_config or \"update_all\" in other_config) else \"\"},{p_value < 0.05}')\n",
    "                    f.write('\\n')\n",
    "                    \n",
    "            f.write('')\n",
    "\n",
    "\n",
    "for env in envs:\n",
    "    for alg in algs:\n",
    "        for sampling in samplings:\n",
    "            for policy in policies:\n",
    "                for detection in detections:\n",
    "                    for reevaluation in reevaluations:\n",
    "                        values = processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                            & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                            & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)]\n",
    "                        if len(values) == 0: continue\n",
    "                        stats_testing_values['__'.join([env, alg, sampling, policy, detection, reevaluation])] = {metric: [float(a.replace('(', '').replace(')','').split(', ')[0]) for a in processed_df.loc[(processed_df['Environment'] == env) & (processed_df['Algorithm'] == alg) \\\n",
    "                                                                                                                       & (processed_df['Policy'] == policy) & (processed_df['Sampling'] == sampling) \\\n",
    "                                                                                                                       & (processed_df['Detection'] == detection) & (processed_df['Re-Evaluation'] == reevaluation)][metric].values[:]] for metric in metrics_of_interest}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75621684a4938efc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for k, res in stats_testing_values.items():\n",
    "    for m, vs in res.items():\n",
    "        print(k, m, 1.96 * np.std(vs) / np.sqrt(len(vs)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ed80d75742e0f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd43e4da4bbcd9b9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
